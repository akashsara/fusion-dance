{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "worst-point",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "from pytorch_msssim import ssim\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import animation \n",
    "from PIL import Image\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import os\n",
    "\n",
    "from IPython.display import HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "biblical-intelligence",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cleared-decimal",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-4\n",
    "epochs = 250\n",
    "batch_size = 128\n",
    "\n",
    "experiment_name = f\"convae_v1\"\n",
    "\n",
    "num_layers = 2\n",
    "max_filters = 1024\n",
    "image_size = 32\n",
    "\n",
    "data_prefix = \"data\"\n",
    "train_data_folder = data_prefix + \"/train/\"\n",
    "val_data_folder = data_prefix + \"/val/\"\n",
    "test_data_folder = data_prefix + \"/test/\"\n",
    "\n",
    "output_prefix = f\"outputs/{experiment_name}\"\n",
    "output_dir = output_prefix + \"/generated\"\n",
    "model_output_path = output_prefix + \"/model.pt\"\n",
    "animation_output_path = output_prefix + \"/animation.mp4\"\n",
    "loss_output_path = output_prefix + \"/loss.jpg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "common-enough",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if gpu else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "retained-lesson",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gpu, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "informed-producer",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "iraqi-judge",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "_ = torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "killing-performance",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "needed-eating",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images_from_folder(folder):\n",
    "    dataset = {}\n",
    "    for file in os.listdir(folder):\n",
    "        image = Image.open(os.path.join(folder, file))\n",
    "        dataset[file] = np.array(image)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "smoking-abuse",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = load_images_from_folder(train_data_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "marine-above",
   "metadata": {},
   "outputs": [],
   "source": [
    "val = load_images_from_folder(val_data_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "honey-aside",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = load_images_from_folder(test_data_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "romantic-virginia",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train), len(val), len(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "desirable-commercial",
   "metadata": {},
   "source": [
    "# Visualize Some Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "returning-hampton",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    \"train\": list(train.values()),\n",
    "    \"test\": list(test.values()),\n",
    "    \"val\": list(val.values())\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "weird-franchise",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, dataset in enumerate(data.keys()):\n",
    "    for j in range(2):\n",
    "        index = np.random.randint(0, len(data[dataset]))\n",
    "        axes = plt.subplot(2, 3, i+j*3+1)\n",
    "        plt.imshow(data[dataset][index])\n",
    "        axes.set_title(f\"{dataset} ({index})\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wired-fashion",
   "metadata": {},
   "source": [
    "# Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stable-nylon",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize(image_size, interpolation=transforms.InterpolationMode.BICUBIC),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "amber-doubt",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = [transform(x) for name, x in train.items()]\n",
    "val_data = [transform(x) for name, x in val.items()]\n",
    "test_data = [(name, transform(x)) for name, x in test.items()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "graphic-fields",
   "metadata": {},
   "source": [
    "# Setup Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "automated-think",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    train_data, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=gpu\n",
    ")\n",
    "val_dataloader = torch.utils.data.DataLoader(\n",
    "    val_data, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=gpu\n",
    ")\n",
    "test_dataloader = torch.utils.data.DataLoader(\n",
    "    test_data, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=gpu\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "improved-scholarship",
   "metadata": {},
   "source": [
    "# Visualize Again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unexpected-boring",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_grid(images, height, width, axis):\n",
    "    i, j = 0, 0\n",
    "    text, images = images\n",
    "    for num, image in enumerate(images):\n",
    "        if num == height * width:\n",
    "            break\n",
    "        axis[i,j].imshow(np.asarray(image.permute(1, 2, 0)))\n",
    "        if j == width - 1:\n",
    "            j = 0\n",
    "            i += 1\n",
    "        else:\n",
    "            j += 1\n",
    "    if type(text) == int:\n",
    "        text = f\"Epoch: {text}\"\n",
    "    fig.suptitle(text, va=\"baseline\")\n",
    "    plt.tight_layout()\n",
    "    return axis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "urban-scanner",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_samples_from_data(data, sample_size, test=False):\n",
    "    sample = []\n",
    "    for i in np.random.choice(len(data), size=sample_size, replace=False):\n",
    "        if test:\n",
    "            sample.append(np.asarray(data[i][1]))\n",
    "        else:\n",
    "            sample.append(np.asarray(data[i]))\n",
    "    return torch.as_tensor(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "academic-passion",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a sample set that we visualize every epoch to show the model's training\n",
    "sample = get_samples_from_data(val_data, 16)\n",
    "test_sample = get_samples_from_data(test_data, 16, test=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "therapeutic-adrian",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axis = plt.subplots(4, 4, figsize=(8, 6), dpi=80)\n",
    "plt.tight_layout()\n",
    "_ = make_grid((\"Sample\", sample), 4, 4, axis)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "premier-marathon",
   "metadata": {},
   "source": [
    "# Model Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expressed-exception",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ref: https://github.com/sksq96/pytorch-vae/blob/master/vae-cnn.ipynb\n",
    "class ConvolutionalVAE(nn.Module):\n",
    "    def __init__(self, image_channels=3, max_filters=512, num_layers=4, kernel_size=2, stride=2, \n",
    "                 padding=0, latent_dim=128, input_image_dimensions=96):\n",
    "        super(ConvolutionalVAE, self).__init__()\n",
    "        channel_sizes = self.calculate_channel_sizes(image_channels, max_filters, num_layers)\n",
    "        # Encoder\n",
    "        encoder_layers = nn.ModuleList()\n",
    "        for i, channel_size in enumerate(channel_sizes):\n",
    "            in_channels = channel_size[0]\n",
    "            out_channels = channel_size[1]\n",
    "            # Convolutional Layer\n",
    "            encoder_layers.append(\n",
    "                nn.Conv2d(\n",
    "                    in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, \n",
    "                    stride=stride, padding=padding, bias=False\n",
    "                )\n",
    "            )\n",
    "            # Batch Norm\n",
    "            encoder_layers.append(nn.BatchNorm2d(out_channels))\n",
    "            # ReLU\n",
    "            encoder_layers.append(nn.ReLU())\n",
    "        # Flatten Encoder Output\n",
    "        encoder_layers.append(nn.Flatten())\n",
    "        self.encoder = nn.Sequential(*encoder_layers)\n",
    "        \n",
    "        # Calculate shape of the flattened image\n",
    "        hidden_dim, image_size = self.get_flattened_size(kernel_size, stride, max_filters, input_image_dimensions)\n",
    "        \n",
    "        # Latent Space\n",
    "        self.fc_mu = nn.Linear(hidden_dim, latent_dim)\n",
    "        self.fc_log_var = nn.Linear(hidden_dim, latent_dim)\n",
    "        \n",
    "        # Decoder\n",
    "        decoder_layers = nn.ModuleList()\n",
    "        # Feedforward/Dense Layer to expand our latent dimensions\n",
    "        decoder_layers.append(nn.Linear(latent_dim, hidden_dim))\n",
    "        # Unflatten to a shape of (Channels, Height, Width)\n",
    "        decoder_layers.append(nn.Unflatten(1, (max_filters, image_size, image_size)))\n",
    "        for i, channel_size in enumerate(channel_sizes[::-1]):\n",
    "            in_channels = channel_size[1]\n",
    "            out_channels = channel_size[0]\n",
    "            # Add Transposed Convolutional Layer\n",
    "            decoder_layers.append(\n",
    "                nn.ConvTranspose2d(\n",
    "                    in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, \n",
    "                    stride=stride, padding=padding, bias=False\n",
    "                )\n",
    "            )\n",
    "            # Batch Norm\n",
    "            decoder_layers.append(nn.BatchNorm2d(out_channels))\n",
    "            # ReLU if not final layer\n",
    "            if i != num_layers - 1:\n",
    "                decoder_layers.append(nn.ReLU())\n",
    "            # Sigmoid if final layer\n",
    "            else:\n",
    "                decoder_layers.append(nn.Sigmoid())\n",
    "        self.decoder = nn.Sequential(*decoder_layers) \n",
    "        \n",
    "    def get_flattened_size(self, kernel_size, stride, filters, input_image_dimensions):\n",
    "        x = input_image_dimensions\n",
    "        for layer in self.encoder:\n",
    "            if \"Conv2d\" in str(layer):\n",
    "                x = ((x - kernel_size) // stride) + 1\n",
    "        return filters * x * x, x\n",
    "    \n",
    "    def calculate_channel_sizes(self, image_channels, max_filters, num_layers):\n",
    "        channel_sizes = [(image_channels, max_filters // np.power(2, num_layers - 1))]\n",
    "        for i in range(1, num_layers):\n",
    "            prev = channel_sizes[-1][-1]\n",
    "            new = prev * 2\n",
    "            channel_sizes.append((prev, new))\n",
    "        return channel_sizes\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Encode\n",
    "        hidden_state = self.encoder(x)\n",
    "        # Reparameterize\n",
    "        mu = self.fc_mu(hidden_state) \n",
    "        log_var = self.fc_log_var(hidden_state)\n",
    "        z = self.reparameterize(mu, log_var)\n",
    "        # Decode\n",
    "        reconstructed = self.decoder(z)\n",
    "        return reconstructed, mu, log_var\n",
    "    \n",
    "    def reparameterize(self, mu, log_var):\n",
    "        std = torch.exp(log_var.mul(0.5)) # log sqrt(x) = log x^0.5 = 0.5 log x\n",
    "        epsilon = torch.randn_like(mu)\n",
    "        z = mu + (epsilon * std)\n",
    "        return z"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pressing-enlargement",
   "metadata": {},
   "source": [
    "# Loss Function\n",
    "\n",
    "Since this is a VAE, we also want to minimize the KL-Divergence between the latent vector Z and our input distribution.\n",
    "So we add the reconstruction loss + KL-Divergence to get our total loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "respected-assembly",
   "metadata": {},
   "outputs": [],
   "source": [
    "def VAELoss(x, reconstructed_x, mu, log_var):\n",
    "#     reconstruction_loss = nn.functional.mse_loss(reconstructed_x, x, reduction='sum')\n",
    "#     KL_d = -0.5 * torch.mean(1 + log_var - mu.pow(2) - log_var.exp())\n",
    "    reconstruction_loss = nn.functional.binary_cross_entropy(reconstructed_x, x, reduction='sum')\n",
    "    KL_d = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
    "    return reconstruction_loss + KL_d, reconstruction_loss, KL_d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proved-terrorism",
   "metadata": {},
   "source": [
    "# Training Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "virtual-pocket",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ConvolutionalVAE(max_filters=max_filters, num_layers=num_layers, input_image_dimensions=image_size)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "authentic-prerequisite",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accepted-height",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "all_samples = []\n",
    "all_train_loss = []\n",
    "all_val_loss = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_loss = 0\n",
    "    train_recon_loss = 0\n",
    "    train_kl_d = 0\n",
    "    val_loss = 0\n",
    "    val_recon_loss = 0\n",
    "    val_kl_d = 0\n",
    "    \n",
    "    # Training Loop\n",
    "    for iteration, batch in enumerate(tqdm(train_dataloader)):\n",
    "        # Reset gradients back to zero for this iteration\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Move batch to device\n",
    "        batch = batch.to(device)\n",
    "\n",
    "        # Run our model & get outputs\n",
    "        reconstructed, mu, log_var = model(batch)\n",
    "\n",
    "        # Calculate reconstruction loss\n",
    "        batch_loss, batch_recon_loss, batch_kl_d = VAELoss(batch, reconstructed, mu, log_var)\n",
    "                  \n",
    "        # Backprop\n",
    "        batch_loss.backward()\n",
    "        \n",
    "        # Update our optimizer parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Add the batch's loss to the total loss for the epoch\n",
    "        train_loss += batch_loss.item()\n",
    "        train_recon_loss += batch_recon_loss.item()\n",
    "        train_kl_d += batch_kl_d.item()\n",
    "        \n",
    "    # Validation Loop\n",
    "    with torch.no_grad():\n",
    "        for iteration, batch in enumerate(tqdm(val_dataloader)):\n",
    "            # Move batch to device\n",
    "            batch = batch.to(device)\n",
    "\n",
    "            # Run our model & get outputs\n",
    "            reconstructed, mu, log_var = model(batch)\n",
    "\n",
    "            # Calculate reconstruction loss\n",
    "            batch_loss, batch_recon_loss, batch_kl_d = VAELoss(batch, reconstructed, mu, log_var)\n",
    "\n",
    "            # Add the batch's loss to the total loss for the epoch\n",
    "            val_loss += batch_loss.item()\n",
    "            val_recon_loss += batch_recon_loss.item()\n",
    "            val_kl_d += batch_kl_d.item()\n",
    "\n",
    "        # Get reconstruction of our sample\n",
    "        epoch_sample, _, _ = model(sample.to(device))\n",
    "\n",
    "    # Add sample reconstruction to our list\n",
    "    all_samples.append(epoch_sample.detach().cpu())\n",
    "    \n",
    "    # Compute the average losses for this epoch\n",
    "    train_loss = train_loss / len(train_dataloader)\n",
    "    train_recon_loss = train_recon_loss / len(train_dataloader)\n",
    "    train_kl_d = train_kl_d / len(train_dataloader)\n",
    "    all_train_loss.append((train_loss, train_recon_loss, train_kl_d))\n",
    "    \n",
    "    val_loss = val_loss / len(val_dataloader)\n",
    "    val_recon_loss = val_recon_loss / len(val_dataloader)\n",
    "    val_kl_d = val_kl_d / len(val_dataloader)\n",
    "    all_val_loss.append((val_loss, val_recon_loss, val_kl_d))\n",
    "    \n",
    "    # Print Metrics\n",
    "    print(\n",
    "        f\"Epoch: {epoch+1}/{epochs}:\\\n",
    "        \\nTrain Loss = {train_loss}, Train Reconstruction Loss = {train_recon_loss}, Train KL Divergence = {train_kl_d}\\\n",
    "        \\nVal Loss = {val_loss}, Val Reconstruction Loss = {val_recon_loss}, Val KL Divergence = {val_kl_d}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "partial-building",
   "metadata": {},
   "source": [
    "# Visualize Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accessible-grenada",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot Original Image\n",
    "fig, axis = plt.subplots(4, 4, figsize=(8, 6), dpi=80)\n",
    "plt.tight_layout()\n",
    "_ = make_grid((\"Sample\", sample), 4, 4, axis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tough-wales",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, axis = plt.subplots(4, 4, figsize=(8, 6), dpi=80)\n",
    "plt.tight_layout()\n",
    "plt.close()\n",
    "anim = animation.FuncAnimation(fig=fig, func=make_grid, frames=list(enumerate(all_samples)), \n",
    "                               fargs=(4, 4, axis), interval=100, repeat=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "played-great",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "HTML(anim.to_html5_video())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lovely-mention",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "covered-bedroom",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dense-escape",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_inputs = []\n",
    "all_outputs = []\n",
    "file_names = []\n",
    "\n",
    "# Testing Loop\n",
    "with torch.no_grad():\n",
    "    for iteration, batch in enumerate(tqdm(test_dataloader)):\n",
    "        # Move batch to device\n",
    "        filename, image = batch\n",
    "        image = image.to(device)\n",
    "\n",
    "        # Run our model & get outputs\n",
    "        reconstructed, _, _ = model(image)\n",
    "\n",
    "        all_inputs.extend(image.detach().cpu().numpy())\n",
    "        all_outputs.extend(reconstructed.detach().cpu().numpy())\n",
    "        file_names.extend(filename)\n",
    "        \n",
    "all_inputs = torch.as_tensor(all_inputs)\n",
    "all_outputs = torch.as_tensor(all_outputs)\n",
    "\n",
    "mse = nn.functional.mse_loss(all_outputs, all_inputs)\n",
    "ssim_score = ssim(all_outputs, all_inputs, data_range=1.0, win_size=11, win_sigma=1.5, K=(0.01, 0.03))\n",
    "\n",
    "# Print Metrics\n",
    "print(\n",
    "    f\"MSE = {mse}, SSIM = {ssim_score}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "operating-blond",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot A Set of Test Images\n",
    "fig, axis = plt.subplots(4, 4, figsize=(8, 6), dpi=80)\n",
    "plt.tight_layout()\n",
    "_ = make_grid((\"Test Sample\", test_sample), 4, 4, axis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "christian-cylinder",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    reconstructed = model(test_sample)[0].detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "molecular-castle",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot A Set of Test Images\n",
    "fig, axis = plt.subplots(4, 4, figsize=(8, 6), dpi=80)\n",
    "plt.tight_layout()\n",
    "_ = make_grid((\"Reconstructed Test\", reconstructed), 4, 4, axis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "recent-morris",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6), dpi=100)\n",
    "ax = plt.subplot()\n",
    "plt.plot([x[1] for x in all_train_loss], label=\"Train Reconstruction Loss\")\n",
    "plt.plot([x[2] for x in all_train_loss], label=\"Train KL-Divergence\")\n",
    "plt.plot([x[1] for x in all_val_loss], label=\"Validation Reconstruction Loss\")\n",
    "plt.plot([x[2] for x in all_val_loss], label=\"Validation KL-Divergence\")\n",
    "ax.set_xlabel(\"Epoch\")\n",
    "ax.set_ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(loss_output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "artificial-usage",
   "metadata": {},
   "source": [
    "# Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vocational-costume",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), model_output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "computational-absence",
   "metadata": {},
   "source": [
    "# Save Generated Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "casual-diana",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_outputs = all_outputs.permute(0, 2, 3, 1).numpy()\n",
    "for image, name in zip(all_outputs, file_names):\n",
    "    plt.imsave(os.path.join(output_dir, name), image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "powerful-praise",
   "metadata": {},
   "source": [
    "# Save Animation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "likely-chair",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "Writer = animation.writers['ffmpeg']\n",
    "writer = Writer()\n",
    "anim.save(animation_output_path, writer=writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incorporate-jurisdiction",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "placed-italic",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "japanese-contest",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}