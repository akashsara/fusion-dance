{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "attempted-induction",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import animation \n",
    "from PIL import Image\n",
    "\n",
    "from models import ConvolutionalAE, ConvolutionalVAE\n",
    "\n",
    "import os\n",
    "\n",
    "from IPython.display import HTML\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "victorian-programmer",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if gpu else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "proved-employee",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gpu, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "early-breast",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "otherwise-princess",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name = f\"convolutional_vae_v2\"\n",
    "model_type = \"vae\" if \"vae\" in experiment_name else \"autoencoder\"\n",
    "\n",
    "num_layers = 4\n",
    "max_filters = 512\n",
    "batch_size = 64\n",
    "image_size = 64\n",
    "latent_dim = 1024\n",
    "\n",
    "data_prefix = f\"\"\n",
    "data_folder = data_prefix + \"../original_data/\"\n",
    "\n",
    "model_prefix = f\"\"\n",
    "model_path = model_prefix + f\"../outputs/{experiment_name}/model.pt\"\n",
    "\n",
    "output_prefix = f\"\"\n",
    "output_dir = output_prefix + f\"{experiment_name}_fusions\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "regulation-response",
   "metadata": {},
   "source": [
    "# Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "divided-rental",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_image(image, model_output=False):\n",
    "    if model_output:\n",
    "        image = image.detach().squeeze(0)\n",
    "    plt.imshow(image.permute(1, 2, 0).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "variable-shareware",
   "metadata": {},
   "outputs": [],
   "source": [
    "def animate(image, axis):\n",
    "    axis.imshow(image.permute(1, 2, 0).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "remarkable-wilderness",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_animation(frames):\n",
    "    fig, axis = plt.subplots(1, 1, dpi=80)\n",
    "    plt.tight_layout()\n",
    "    plt.close()\n",
    "    return animation.FuncAnimation(fig=fig, func=animate, frames=frames, fargs=(axis, ), interval=300, repeat=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "charged-oliver",
   "metadata": {},
   "source": [
    "# Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rising-warrant",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_type, max_filters, num_layers, input_image_dimensions, latent_dim, model_path, device):\n",
    "    if model_type == \"autoencoder\":\n",
    "        model = ConvolutionalAE(max_filters=max_filters, num_layers=num_layers, input_image_dimensions=image_size, \n",
    "                                latent_dim=latent_dim)\n",
    "    else:\n",
    "        model = ConvolutionalVAE(max_filters=max_filters, num_layers=num_layers, input_image_dimensions=image_size, \n",
    "                                 latent_dim=latent_dim)\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model.eval()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "light-budget",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(model_type, max_filters, num_layers, image_size, latent_dim, model_path, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ruled-coordinator",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collectible-wedding",
   "metadata": {},
   "source": [
    "# Load & Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "british-capacity",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize(image_size, interpolation=transforms.InterpolationMode.BICUBIC),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sorted-touch",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_folder, image_size, transform):\n",
    "    pokemon = {}\n",
    "    background = Image.new(\"RGBA\", (image_size, image_size), (255, 255, 255))\n",
    "    for file in os.listdir(data_folder):\n",
    "        id_ = file.split('.')[0].split('_')[0].split('-')[0]\n",
    "        form = '_'.join(file.split('.')[0].split('_')[1:])\n",
    "        image = Image.open(os.path.join(data_folder, file))\n",
    "        image = image.resize((image_size, image_size), resample=Image.BICUBIC).convert(\"RGBA\")\n",
    "        image = Image.alpha_composite(background, image).convert(\"RGB\")\n",
    "        if id_ not in pokemon:\n",
    "            pokemon[id_] = {}\n",
    "        pokemon[id_][form] = transform(image)\n",
    "    return pokemon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continent-paradise",
   "metadata": {},
   "outputs": [],
   "source": [
    "pokemon = load_data(data_folder, image_size, transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "regular-collapse",
   "metadata": {},
   "outputs": [],
   "source": [
    "image1 = pokemon['004']['base_hgss']\n",
    "image2 = pokemon['025']['base_hgss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cardiac-grade",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig=plt.figure(figsize=(8, 8))\n",
    "fig.add_subplot(1, 2, 1)\n",
    "show_image(image1)\n",
    "fig.add_subplot(1, 2, 2)\n",
    "show_image(image2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arranged-initial",
   "metadata": {},
   "source": [
    "# Fusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bacterial-ethernet",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fusion_add(image1, image2, weights=(1,1)):\n",
    "    image1 = image1.clone()\n",
    "    image2 = image2.clone()\n",
    "    return torch.add(torch.mul(image1, weights[0]), torch.mul(image2, weights[1])) / (weights[0] + weights[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "polar-filling",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fusion_mix_match(image1, image2, num_dims):\n",
    "    image1 = image1.clone()\n",
    "    image2 = image2.clone()\n",
    "    feature_dims = np.random.randint(0, image1.shape[1], num_dims)\n",
    "    image1[0, feature_dims] = image2[0, feature_dims]\n",
    "    return image1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "silver-penalty",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_latent_representations(image1, image2, model_type, model):\n",
    "    if model_type == \"autoencoder\":\n",
    "        output1 = model.encoder(image1.unsqueeze(0)).detach()\n",
    "        output2 = model.encoder(image2.unsqueeze(0)).detach()\n",
    "        return output1, output2\n",
    "    else:\n",
    "        _, output1_mu, output1_log_var = model(image1.unsqueeze(0))\n",
    "        _, output2_mu, output2_log_var = model(image2.unsqueeze(0))\n",
    "        output1_mu = output1_mu.detach()\n",
    "        output1_log_var = output1_log_var.detach()\n",
    "        output2_mu = output2_mu.detach()\n",
    "        output2_log_var = output2_log_var.detach()\n",
    "        return output1_mu, output1_log_var, output2_mu, output2_log_var"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "resident-press",
   "metadata": {},
   "source": [
    "## Add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "referenced-steering",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Individual\n",
    "weights = (0.5, 0.5)\n",
    "if model_type == \"autoencoder\":\n",
    "    output1, output2 = get_latent_representations(image1, image2, model_type, model)\n",
    "    combined = fusion_add(output1, output2, weights=weights)\n",
    "else:\n",
    "    output1_mu, output1_log_var, output2_mu, output2_log_var = get_latent_representations(image1, image2, model_type, model)\n",
    "    combined_mu = fusion_add(output1_mu, output2_mu, weights=weights)\n",
    "    combined_log_var = fusion_add(output1_log_var, output2_log_var, weights=weights)\n",
    "    combined = model.reparameterize(combined_mu, combined_log_var)\n",
    "show_image(model.decoder(combined), model_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "corporate-method",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Animation\n",
    "to_animate = []\n",
    "for i in np.arange(0.0, 1.0, 0.1):\n",
    "    if model_type == \"autoencoder\":\n",
    "        combined = fusion_add(output1, output2, weights=(i, 1-i))\n",
    "    else:\n",
    "        output1_mu, output1_log_var, output2_mu, output2_log_var = get_latent_representations(\n",
    "            image1, image2, model_type, model\n",
    "        )\n",
    "        combined_mu = fusion_add(output1_mu, output2_mu, weights=(i, 1-i))\n",
    "        combined_log_var = fusion_add(output1_log_var, output2_log_var, weights=(i, 1-i))\n",
    "        combined = model.reparameterize(combined_mu, combined_log_var)\n",
    "    out = model.decoder(combined).detach().squeeze(0)\n",
    "    to_animate.append(out)\n",
    "anim = create_animation(to_animate)\n",
    "HTML(anim.to_html5_video())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "posted-optimum",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Bulk\n",
    "weights = (0.5, 0.5)\n",
    "spritesheet = 'base_hgss'\n",
    "new_output_dir = output_dir + '_add'\n",
    "if not os.path.exists(new_output_dir):\n",
    "    os.mkdir(new_output_dir)\n",
    "already_done = os.listdir(new_output_dir)\n",
    "\n",
    "for base in tqdm(range(1, 152)):\n",
    "    for fusion in tqdm(range(1, 152), leave=False):\n",
    "        \n",
    "        # Skip if already done\n",
    "        file_name = f\"{base}.{fusion}.png\"\n",
    "        if file_name in already_done:\n",
    "            continue\n",
    "        \n",
    "        # Get images\n",
    "        image1 = pokemon[f\"{base:03}\"][spritesheet]\n",
    "        image2 = pokemon[f\"{fusion:03}\"][spritesheet]\n",
    "        \n",
    "        # Get representations\n",
    "        if model_type == \"autoencoder\":\n",
    "            output1, output2 = get_latent_representations(image1, image2, model_type, model)\n",
    "        else:\n",
    "            output1_mu, output1_log_var, output2_mu, output2_log_var = get_latent_representations(\n",
    "                image1, image2, model_type, model\n",
    "            )\n",
    "            \n",
    "        # Combine representations\n",
    "        if model_type == \"autoencoder\":\n",
    "            combined = fusion_add(output1, output2, weights=weights)\n",
    "        else:\n",
    "            combined_mu = fusion_add(output1_mu, output2_mu, weights=weights)\n",
    "            combined_log_var = fusion_add(output1_log_var, output2_log_var, weights=weights)\n",
    "            combined = model.reparameterize(combined_mu, combined_log_var)\n",
    "            \n",
    "        # Decode representation\n",
    "        out = model.decoder(combined).detach().squeeze(0).permute(1, 2, 0).numpy() * 255\n",
    "        \n",
    "        # Save representation\n",
    "        Image.fromarray(out.astype(np.uint8)).save(os.path.join(new_output_dir, file_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tutorial-chaos",
   "metadata": {},
   "source": [
    "## Recombine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "employed-oasis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single\n",
    "num_dims = 512\n",
    "if model_type == \"autoencoder\":\n",
    "    output1 = model.encoder(image1.unsqueeze(0)).detach()\n",
    "    output2 = model.encoder(image2.unsqueeze(0)).detach()\n",
    "    combined = fusion_mix_match(output1, output2, num_dims=num_dims)\n",
    "else:\n",
    "    output1_mu, output1_log_var, output2_mu, output2_log_var = get_latent_representations(\n",
    "        image1, image2, model_type, model\n",
    "    )\n",
    "    \n",
    "    combined_mu = fusion_mix_match(output1_mu, output2_mu, num_dims=num_dims)\n",
    "    combined_log_var = fusion_mix_match(output1_log_var, output2_log_var, num_dims=num_dims)\n",
    "    combined = model.reparameterize(combined_mu, combined_log_var)\n",
    "show_image(model.decoder(combined), model_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "electronic-causing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Animation\n",
    "to_animate = []\n",
    "for num_dims in [2, 4, 8, 16, 32, 64, 128, 256, 512, 768, 1024, 1596, 2048]:\n",
    "    if model_type == \"autoencoder\":\n",
    "        output1 = model.encoder(image1.unsqueeze(0)).detach()\n",
    "        output2 = model.encoder(image2.unsqueeze(0)).detach()\n",
    "        combined = fusion_mix_match(output1, output2, num_dims=num_dims)\n",
    "    else:\n",
    "        output1_mu, output1_log_var, output2_mu, output2_log_var = get_latent_representations(\n",
    "            image1, image2, model_type, model\n",
    "        )\n",
    "\n",
    "        combined_mu = fusion_mix_match(output1_mu, output2_mu, num_dims=num_dims)\n",
    "        combined_log_var = fusion_mix_match(output1_log_var, output2_log_var, num_dims=num_dims)\n",
    "        combined = model.reparameterize(combined_mu, combined_log_var)\n",
    "    out = model.decoder(combined).detach().squeeze(0)\n",
    "    to_animate.append(out)\n",
    "anim = create_animation(to_animate)\n",
    "HTML(anim.to_html5_video())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collectible-plaza",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bulk\n",
    "num_dims = latent_dim // 2\n",
    "spritesheet = 'base_hgss'\n",
    "new_output_dir = output_dir + '_mix_match'\n",
    "if not os.path.exists(new_output_dir):\n",
    "    os.mkdir(new_output_dir)\n",
    "already_done = os.listdir(new_output_dir)\n",
    "\n",
    "for base in tqdm(range(1, 152)):\n",
    "    for fusion in tqdm(range(1, 152), leave=False):\n",
    "        \n",
    "        # Skip if already done\n",
    "        file_name = f\"{base}.{fusion}.png\"\n",
    "        if file_name in already_done:\n",
    "            continue\n",
    "        \n",
    "        # Get images\n",
    "        image1 = pokemon[f\"{base:03}\"][spritesheet]\n",
    "        image2 = pokemon[f\"{fusion:03}\"][spritesheet]\n",
    "        \n",
    "        # Get representations\n",
    "        if model_type == \"autoencoder\":\n",
    "            output1 = model.encoder(image1.unsqueeze(0)).detach()\n",
    "            output2 = model.encoder(image2.unsqueeze(0)).detach()\n",
    "        else:\n",
    "            output1_mu, output1_log_var, output2_mu, output2_log_var = get_latent_representations(\n",
    "                image1, image2, model_type, model\n",
    "            )\n",
    "            \n",
    "        # Combine representations\n",
    "        if model_type == \"autoencoder\":\n",
    "            combined = fusion_mix_match(output1, output2, num_dims=num_dims)\n",
    "        else:\n",
    "            combined_mu = fusion_mix_match(output1_mu, output2_mu, num_dims=num_dims)\n",
    "            combined_log_var = fusion_mix_match(output1_log_var, output2_log_var, num_dims=num_dims)\n",
    "            combined = model.reparameterize(combined_mu, combined_log_var)\n",
    "            \n",
    "        # Decode representation\n",
    "        out = model.decoder(combined).detach().squeeze(0).permute(1, 2, 0).numpy() * 255\n",
    "        \n",
    "        # Save representation\n",
    "        Image.fromarray(out.astype(np.uint8)).save(os.path.join(new_output_dir, file_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "radical-poker",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
