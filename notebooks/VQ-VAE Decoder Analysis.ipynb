{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "gross-australian",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pytorch_msssim\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "import utils.data as data\n",
    "import utils.graphics as graphics\n",
    "import utils.loss as loss\n",
    "import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ordinary-session",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "_ = torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "animal-effort",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "num_dataloader_workers = 0\n",
    "\n",
    "experiment_name = f\"vq_vae_v5.16\"\n",
    "\n",
    "num_layers = 2\n",
    "num_embeddings = 512\n",
    "embedding_dim = 64\n",
    "commitment_cost = 0.25\n",
    "use_max_filters = True\n",
    "max_filters = 512\n",
    "image_size = 64\n",
    "use_noise_images = True\n",
    "small_conv = True  # To use the 1x1 convolution layer\n",
    "encoding_dim = image_size // (2 ** num_layers)\n",
    "\n",
    "data_folder = \"..\\\\data\\\\Pokemon\\\\original_data\"\n",
    "\n",
    "output_dir = f\"..\\\\outputs\\\\{experiment_name}\"\n",
    "model_path = os.path.join(output_dir, \"model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "mature-canvas",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False cpu\n"
     ]
    }
   ],
   "source": [
    "# Setup Device\n",
    "gpu = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if gpu else \"cpu\")\n",
    "print(gpu, device)\n",
    "\n",
    "# Create Output Paths\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "periodic-footwear",
   "metadata": {},
   "source": [
    "# Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "played-serve",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VQVAE(\n",
       "  (encoder): Sequential(\n",
       "    (0): Conv2d(3, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): Conv2d(128, 256, kernel_size=(2, 2), stride=(2, 2), bias=False)\n",
       "    (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): ReLU()\n",
       "    (6): Conv2d(256, 512, kernel_size=(2, 2), stride=(2, 2), bias=False)\n",
       "    (7): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (8): ReLU()\n",
       "    (9): Conv2d(512, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "  )\n",
       "  (vq_vae): VectorQuantizerEMA(\n",
       "    (_embedding): Embedding(512, 64)\n",
       "  )\n",
       "  (decoder): Sequential(\n",
       "    (0): Conv2d(64, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (1): ConvTranspose2d(512, 256, kernel_size=(2, 2), stride=(2, 2), bias=False)\n",
       "    (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (3): ReLU()\n",
       "    (4): ConvTranspose2d(256, 128, kernel_size=(2, 2), stride=(2, 2), bias=False)\n",
       "    (5): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (6): ReLU()\n",
       "    (7): ConvTranspose2d(128, 3, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (8): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (9): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = models.VQVAE(\n",
    "    num_layers=num_layers,\n",
    "    input_image_dimensions=image_size,\n",
    "    small_conv=small_conv,\n",
    "    embedding_dim=embedding_dim,\n",
    "    num_embeddings=num_embeddings,\n",
    "    commitment_cost=commitment_cost,\n",
    "    use_max_filters=use_max_filters,\n",
    "    max_filters=max_filters,\n",
    ")\n",
    "model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "model.eval()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "distinguished-syndrome",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "certain-generator",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset_directory, transform):\n",
    "        self.dataset_path = dataset_directory\n",
    "        self.all_images = os.listdir(dataset_directory)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        filename = self.all_images[index]\n",
    "        image_path = os.path.join(self.dataset_path, filename)\n",
    "        image = Image.open(image_path).convert(\"RGBA\")\n",
    "        background = Image.new(\"RGBA\", image.size, (255, 255, 255))\n",
    "        image = Image.alpha_composite(background, image).convert(\"RGB\")\n",
    "        fusion = self.transform(image)\n",
    "        return filename, fusion\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.all_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "handed-freeze",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = data.image2tensor_resize(image_size)\n",
    "dataset = CustomDataset(data_folder, transform)\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "    dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=num_dataloader_workers,\n",
    "    pin_memory=gpu,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "flexible-questionnaire",
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_from_array(image, model_output=False):\n",
    "    if model_output:\n",
    "        image = image.detach().squeeze(0)\n",
    "    plt.imshow(image.permute(1, 2, 0).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "constitutional-investigation",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_shape = (1, encoding_dim, encoding_dim, embedding_dim,)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tight-regression",
   "metadata": {},
   "source": [
    "# Get Encoding Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "dental-munich",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_encodings = encoding_dim ** 2\n",
    "mapping_size = image_size // encoding_dim\n",
    "encodings_mapping = {}\n",
    "for i in range(num_embeddings):\n",
    "    image = torch.tensor([i] * num_encodings).unsqueeze(1)\n",
    "    model_output = model.quantize_and_decode(image, target_shape, device).permute(0, 2, 3, 1)[0].detach()\n",
    "    model_output = model_output[0:mapping_size, 0:mapping_size]\n",
    "    encodings_mapping[i] = model_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "cardiac-crowd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.3958, 0.5501, 0.7217],\n",
       "         [0.3938, 0.5406, 0.7250],\n",
       "         [0.3830, 0.5310, 0.7262],\n",
       "         [0.3887, 0.5494, 0.7390]],\n",
       "\n",
       "        [[0.3908, 0.5342, 0.7269],\n",
       "         [0.4040, 0.5600, 0.7414],\n",
       "         [0.3996, 0.5554, 0.7477],\n",
       "         [0.3795, 0.5328, 0.7252]],\n",
       "\n",
       "        [[0.3355, 0.4689, 0.6303],\n",
       "         [0.3535, 0.4800, 0.6509],\n",
       "         [0.3475, 0.4801, 0.6530],\n",
       "         [0.3250, 0.4582, 0.6067]],\n",
       "\n",
       "        [[0.1398, 0.1947, 0.2464],\n",
       "         [0.1301, 0.1703, 0.2148],\n",
       "         [0.1249, 0.1634, 0.2040],\n",
       "         [0.1491, 0.1917, 0.2415]]])"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encodings_mapping[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "objective-budget",
   "metadata": {},
   "source": [
    "# Sample 1 Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "optimum-savage",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encodings_to_image(encodings, mappings, image_size, mapping_size):\n",
    "    image = torch.zeros(image_size, image_size, 3)\n",
    "    j = 0\n",
    "    for i, encoding in enumerate(encodings):\n",
    "        encoding = mappings[encoding.item()]\n",
    "        row_min = (i * mapping_size) % image_size\n",
    "        row_max = row_min + mapping_size\n",
    "        col_min = j * mapping_size\n",
    "        col_max = col_min + mapping_size\n",
    "        image[col_min:col_max, row_min:row_max] = encoding\n",
    "        if row_max == image_size:\n",
    "            j += 1\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "delayed-bundle",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _, x in dataloader:\n",
    "    break\n",
    "x = x[0].unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "portable-tribute",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD7CAYAAACscuKmAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAihUlEQVR4nO2deXhV1dX/vysTIQwyhUEGAxhALBIwyihakIrain2xVPu2osXS1zrg8KqorbWtvrU/rWLtiMVKrZWKoFjqhAxVLCARgkwCAQIEAgnIHMi4f3/cy957XRO4kNyb4Pl+nidPvueudc/Zyc3K2fvstdcWYwwIIV9+Euq7AYSQ+MBgJyQgMNgJCQgMdkICAoOdkIDAYCckINQq2EVklIisF5E8EZlUV40ihNQ9crrz7CKSCGADgJEACgAsA3CDMWZt3TWPEFJXJNXivRcDyDPGbAYAEZkOYDSAGoO9TZs2JiMjoxaXJISciPz8fOzZs0eqs9Um2DsC2O4dFwAYcKI3ZGRkICcnpxaXJISciOzs7BptMX9AJyITRCRHRHKKi4tjfTlCSA3UJth3AOjsHXcKv6YwxkwxxmQbY7LT09NrcTlCSG2oTbAvA5ApIl1FJAXA9QDerJtmEULqmtMesxtjKkTkdgDvAkgE8IIxZk2dtYwQUqfU5gEdjDFvAXirjtpCCIkhzKAjJCAw2AkJCAx2QgICg52QgMBgJyQgMNgJCQgMdkICAoOdkIDAYCckIDDYCQkIDHZCAgKDnZCAwGAnJCAw2AkJCAx2QgICg52QgMBgJyQgMNgJCQgMdkICAoOdkIDAYCckIDDYCQkIDHZCAgKDnZCAwGAnJCCcNNhF5AURKRKR1d5rrURkrohsDH9vGdtmEkJqSzR39hcBjIp4bRKAecaYTADzwseEkAbMSYPdGPMBgM8jXh4NYFpYTwNwbd02ixBS15zumL2dMaYwrHcBaFdH7SGExIhaP6AzxhgApia7iEwQkRwRySkuLq7t5Qghp8npBvtuEekAAOHvRTU5GmOmGGOyjTHZ6enpp3k50hCprKy0X6Thc7rB/iaAcWE9DsDsumkOISRWRDP19gqAxQB6ikiBiIwH8ASAkSKyEcDl4WNCSAMm6WQOxpgbajCNqOO2EEJiyEmDnTQ8zNGjVn+0dKmyDepyjtWJ3brW+lrluavcufv1V7Y3duRb3ensjrW+FoktTJclJCAw2AkJCOzGn4GUrFpjdenC5cpW2W6HOxhZZWXiud2jPr/xptKSOrS1OmfZZ8ovMesCd92ivVGfn9QPvLMTEhAY7IQEBAY7IQGBY/YzkCYXZ1vdfepLyvZpVpbVHy5eZPWwMTcqv47PP2t1e+98AFC+142/d97/mNUZjz6m/A7c+COrK8vKlC0xJaXG9pP6gXd2QgICg52QgMBu/BnIB+++a3XykOHKNuicblaXL99j9aH7dTGhVc3TrC65QXfxW2ReaHXGwz+x2uzRS5STLhnpzrE8V9maDby4xvbXNVVVbooxIYH3r5rgb4aQgMBgJyQgsBsfY0KFfBwiUutzDLviCqu3//BO7TxwiJWDLr/S6jfuHKfcsue2sbrTfb/S52jkusXm0GGrJb2tcpv10G1Wf2fuu4iG0tJSdbx+/XqrDx48qGw//vGPrf7JT9xw4vnnn1d+v//9761u1apVVO0IIryzExIQGOyEBAQGOyEBgWP2E+BP6QA1T+ts2LBBHV9yySVW7969u8bzRxZqTExMtNofp0eO8zffeo/VXSfcrU96rMTpvK1WfpaeodwO/ed9q7/XuomymT1uyk5dOmJM3f5ro60uzdusbI3O7YbqWBpRbGPfvn1WT548Wdl+M3Wq1X98+mmrJ/3yl8pv8jPPWD3uppuUrWtXV8Aj6NNywf7pCQkQDHZCAgK78RH43efIbl/v3r2tfu6556zemJen/Hp5frNmzVK2MWPGVHutSKoqKqx+Z+QVyjaspddF3rVL2cq27rQ6ZYjLYrs583zlVynuZzvy/ofK1mRgP+e3z3XdE5s3Un5H17muu4yJbkqxefPm6vjFF1+0esGCBcr2ziA3jTh+/A+sLrhokPJr/9D9Vt97333K9lfv/JHXDhq8sxMSEBjshAQEBjshAYFj9gj8aa4ZM2Yo28zXXrP6+u+Pt/qff/mz8muf1tjqXuedp2zJyck1Xtsv9HiwYLvVX0nTe+Q169jB6ooFC5Wtaq+b6ps53T1XGNlFt6NpJzclVbp6hbIVeMedOnSyeuyMacrv8ddesDrlnOhq1Gd5xTUAnRL7jd59lO2fQ6+y+sDbH1ndus8Q5dfsZfdc5IZXX1S2YcOGWZ2bmxtVG7+sRLP9U2cRWSAia0VkjYhMDL/eSkTmisjG8PeWsW8uIeR0iaYbXwHgXmNMbwADAdwmIr0BTAIwzxiTCWBe+JgQ0kCJZq+3QgCFYX1IRNYB6AhgNIDLwm7TACwE8EBMWhlHNm7caPXdd+vstJvG3eRsh49Y3ez2Xyi/9M0rrZ4x9hvKtvTjj60+UQbdksGXWn3l2Ft0IyvKrSw/8LkyzV/h6s6NGey6wQb6WnLogNWNIzIFO6W4KbaCxQut3n1gp/LL9LruuyMy49oNGIBo6NbNTSOuy9+ibDu7bLP67B49rD5cckT59UxyW0/1OTdT2cbcGbEqMMCc0gM6EckA0A/AUgDtwv8IAGAXgHZ12zRCSF0SdbCLSFMAMwHcZYxRSdImlB1SbYaIiEwQkRwRySkuLq7OhRASB6IKdhFJRijQXzbGHH/0uVtEOoTtHQAUVfdeY8wUY0y2MSY7PT29OhdCSBw46ZhdQnNRUwGsM8Y87ZneBDAOwBPh77Nj0sI4s22bGyc+5626AoCN33WFGW8ad5fVu1fmKr/lF7g90ObPflPZ7vu5G9/7Y/RI7tlVYPWVEWN7eDXZV675jzIN6OKuDe/0EnEKP1NXUhsr297tbtpvf5FbAXesRQvl9+mNN1m9b5FuR7vNbiXg4sfcz9wpTa+wm9nSTeLklRxWtqKt7rPAz39rZUV5ufJrnOj+jCPXGD77rKuPf6KVhEEgmnn2IQC+B2CViOSGX3sIoSB/VUTGA9gKYGxMWkgIqROieRq/CEBN/wZH1G1zCCGxghl0EYwY4f5/te6us8L23uqyvXbl5lhddr6e7vnGL9wqrLcuGaZsqY3ctFbkqje/WMasLW5F2e0XXab8/ve8r1j9o08+VrbJfVwXv38jl62XnKpXrCV6Xd+8ZTnK1rS1K9rY2Mv4eyf1HOWX1shNwAxt1lTZ5r79ttWD8lyBiuT5f1J+m4e7FX0DfnCrsiVUubHHwwO+avWFEVmJL89/z+ola9comz+9qYZNkSsO/W79iWxnMMyNJyQgMNgJCQjsxp+Ag4cO6Re8Lm17L/PrsNE7mL51+TetHm70k+MkfxHLjkJlU8Uyytz7vrlgjvIbcaHbnmncPfco2x+e+H9WT/yJ60of26mz3xp5T/QTB1+gbL1WbrJ646cuG/Bggv5zWVfu8iZ++a3RypbwopucyV233Or3n/ut8mv81zesXnzLXcq2ee1qqxcddBl/x/K3K78nS1w7NnY/V9lqnPGI7Jpv8p7jd4/ID1NTF2dul553dkICAoOdkIDAYCckIHDMHoE/HVbuZY8BQLc2Lt338XZubNi/dRvll57izjE2+zplu6/QjdPTOndUtpLtO6xuutKNlQ88rBcTbjriliaUP/JTZUve4FafLbvjQavPffxJ5be92BWqvPiVecqGls1QHa2T9b0hY9pfrD723duUbds2V7P+Du+Zxk8jst8eWfSB1bdn91W23n1d4ctN/5lr9fRcXSBz2+fuc/pCEc8Kb0Wf3/zIGvKpXlGR3Hxtu8Cbcjxzh+y8sxMSFBjshAQEduMj8BdIRHYJN+1xUzxpXVxttqMDBiq/FoXeUt7GKco29hNXG/1ogZ4OO+Dppbe6bLLrXpii/HqedZbVqx7+sbKhheuCX/SSN82Vt025tfx4idXLCtcrW+bRs93pGrlFMpcu0dsyLxl8tdWlu/cqW2MvY++Bc9yQ5yuvvqf8KjN7WT100UJl+1n+Oqvv6D7U6lHLdaGMBz39hQUuJcec3up9Li11xh8SvfeVVWibf9woImTOoKk43tkJCQgMdkICAoOdkIDAMfsJOFGBg2uvGGV13ucHlO2twlVW7/lspbL9qINLs516Xj9la+ldb8O6tVb/6y5d+HL9kVKrl9x8s7Jd3tatCPPLMm64Whe+7N4/y+qSPXqKcfE+t+3zR/tcAaInu+upsSbtXeGJVrP+pmxDWzjbwm9PsHpbkU4RPjfRK245SO9p19dLE/51yjLnt0ufo8aVbQDQ3CvM4a0CxK79qJGkiHPkuPRhDO6lbWfOkJ13dkKCAoOdkIDAbvwp4E/FvfK82/Lp10//Wvk1K3ZTaoNSdU32Xi1dtt3CPXr11k96uG7yP693WxQ/nqBX3/35EjcNdcn9P1K2v77pMs0u+uW9VnfeoQoCY+RvJlu9z+hu6ycH3DTaB6PcFtMX3The+c3f66ayHtuuf5YRHbtY/fzid6y+IXu48lu+12UNJpfre09emRuImBV6OHRa+FOpJ7rNRWbhdWzv9O/0Fli4Y1ytmxUveGcnJCAw2AkJCOzGnwI1PZ3v07+/OjZv/tvqIb11HbtP97giCU9tzVO217Pclk9T8lyW2EcTH1N+mTd+x+rFd+m6bWf/zRWvOPzvXNf2qS8ovzRvIcixz3X2W2evsEV2/4utTu2nf87LvUUtSfP/qWy93IQBJm1zi246n52v/NpWuCfkhSX7lO292a+7Nua531XqubpARdTs9X7Opi20bbdXvKJ1a217ZrLTl1x0etduAPDOTkhAYLATEhAY7IQEBI7Z64BLBw9Rx8M3uay29M1rle2Bvm4r43+P/q6y/TPHrYj7npdN1nHAhcrPeI8OBk16VNlee+3vVk9/wY3Tf5upx7lNvJ2W0rvpLZkGp7lxdOKlVzpDe12I8Z1XXnJtrNB16Zt1coU5HsvsbnVls1bK7+5ctwqubaNUZfvTbFekIvu2261evb0ANWEq9VSnlLsVa+ZPbrpU7rlX+eH5552+/XZt2+WN5791tbadQcUoT3pnF5FUEflYRFaKyBoR+Vn49a4islRE8kTkHyKScrJzEULqj2i68aUAhhtj+gLIAjBKRAYC+BWAZ4wx5wLYB2B8zacghNQ30ez1ZgAc7/Qlh78MgOEAjs8BTQPwKIA/1H0TGz4pXj15APhoe77Vi6/5L2Vb+5lb0DGou97GqFma6yavLnX97I4RO5/KJrc11NFDehFO+1mvWP1/w79t9Z7den/TihTX3W2cpP8MWs2bYbV5+Bmry5d+ovyuGeS2ZLpy1j+ULWuOK47Rp7XLQBuTmqb8nunmtrJ6yKsvDwAD33SLazYUu/ZHFhXxp0QlUd+/tvS6zOquV450hqeeUn64806njx7VtiZemw9G7CXQvPp6fQ2RaPdnTwzv4FoEYC6ATQD2G2OOD4gKAHSs4e2EkAZAVMFujKk0xmQB6ATgYgC9TvwOh4hMEJEcEckpLi4++RsIITHhlKbejDH7ASwAMAhACxE53v/rBGBHDe+ZYozJNsZkp6enV+dCCIkD8oU625EOIukAyo0x+0WkMYD3EHo4Nw7ATGPMdBH5I4BPjTG/P9G5srOzTU5OzolcvhT4v9ODx44pW7LXu2lyjt4CedYMN1ae+ANXh33bS28pvyov1XXWk3oK6br+3ri0zI09j0bsW5fkPWfYskZvc7yr1L3v/5q4SZa3btPFLY+VuZ8tLU0XcHw7143ZH5/qxv3FB3VK7B3dzrf6hc/1/WJ5gS6SeTrk93DTohmjLneGioiikn4cPPKINk2darVEFJw09/3Q2WrRzroiOzsbOTk51TYlmnn2DgCmiUgiQj2BV40xc0RkLYDpIvIYgBUApp7oJISQ+iWap/GfAuhXzeubERq/E0LOAJhBFwP8qaCzGjdWtsqObtLiREOoJ+6caPWetZ8q25Eqt9psRHudGWe8raHg1WNr3ERP3/nX7nGRXsnVI8l13VusXGj13CVzld8Vw1w2mSnX21aP7NnH6oG/+J3V37/nJuV3dHe+1ct369pyH4x205b7lq9w53ttuvJrN8BlJUZm0HVc6oZA+394v9UtIp4fmWuusVru1ttgV3i/xz399HRpB5w5MDeekIDAYCckILAbH2e+UOrYw+9aL97pnkxnnZ+l/F49zz3BbnvJ15WtzHv63yh/ozNELtLwjyOe1BuveEWfzGyrk7frp+MtbnP16Vom66URFzZ2T+ffWO+GITmX6JLW03dusXrVxSOUbUhHV3Z7YR/3M7eJKBbiE5lBl9zSbZV11p9cYY8lV12j/AamNbf67cV6uDL0qaet7vAt3X7/MztR6fGGAO/shAQEBjshAYHBTkhA4Ji9AaHGfN5Y8NM1ucpvTg83/dOrdRdlw82ujrnZ6rYt+sKQvcxNlZWO/payNSrMt7pqo1th172Vnq7a+RVXvx4J+r5RZtwU2PRuWVY/t2WZ8puY4Va9HSkrVbZeS/5l9cZCV4v/ZFmfPmpM7Y3fyw4dUX5vzHKFLa7duhHR0tDH6T68sxMSEBjshAQEduPPQFoleNN3o69VNtPYK7TQ1ut2FxUpP3/VhhzRXVoMcd3zhHNc3fs9k59RbqUVLpPPQHetE7xCeWmp7s/sgfUblN/dmQOtzlyzSNnW19B1P5Wus+/74S9+avWwYVcpv7de/0vU5zxT4Z2dkIDAYCckIDDYCQkIHLM3IGqaUiq762F1/LXtW60+vOtzfY6zXNonPneFIqqa6OISiUVeAcd2bfU5St203IEFbhxdcviw8kvw0morS3WRxqqMzlbvbOzqweeM0mmqMz92deM37tqpbFVVbvpOEqK7L0X+Diu9KcDMLW5F4DuFulBGo3fnuHNUViqbnCDF+UyCd3ZCAgKDnZCAwG58A8KfJsqfOMnqjCefUH4v/sdti4QBWcqWcNitYCvt4VaKJZ6vCwJX7nFTcY3Sz9K2w65L3nywK2yRZPQU3bZlLhuuy4T/UbamV3mrw/a7rZKrdukpwAs2uAy9y8d+U9nef9Vt2ex3rU/UrY6cltv03QlWn/2166xu+3e9hXWhV1dfLuhb4/nPZHhnJyQgMNgJCQjsxjcgyotcmemkRl7NuIgEtzGPPWl16Wpdny65c4bVjS512WmIeMKMlu6pvSnXZZUlwXWFj+53i1PKDu5Xfr1fdqWvEVFiGd45UOrOIU10TT4zMMvqyfuvVbYXO7mhx7gVbuhyuEA/tS/f7p6s7/ylzvI7/3W3mAYbnd+gdTpbr3TkDHzZ4Z2dkIDAYCckIDDYCQkIHLM3IDbf4bZy6vmU2/3abNmi/KqaeVNlnyxVtmNJLaxO3LUnqutG1lqvOubG2AkVJc6vrd6uyp9SQ1XNBSX8jDxJ1dtbl23bZXWPnj2Vrft1rqjGkRn/dudr1kj5pTZ3zx/O/9sryobFrt78sDtutHrzMr39dBCI+s4e3rZ5hYjMCR93FZGlIpInIv8QkZSTnYMQUn+cSjd+IoB13vGvADxjjDkXwD4A4+uyYYSQuiWqbryIdAJwNYDHAdwjoTSl4QC+E3aZBuBRAH+o9gQkKtL6unps2LHdSklNVX6Jaa5AxbaCXcrWunCm1QmD3fZMVZG7lnqZZl8oBeEtOqk86mXTXXiB9ivzildU6aGAymTzbEeWrNZ+qa5DmJasu+dpU5+zuuTVD9w5duqpt5SzWll9eFGusu1f57rr8xe5RTfSth2CRrR39skA7gdw/FNrDWC/Meb4X1ABgI7VvI8Q0kA4abCLyNcBFBljTuuJhohMEJEcEckp9vYmJ4TEl2ju7EMAXCMi+QCmI9R9fxZACxE5PgzoBGBHdW82xkwxxmQbY7LTI3bOJITEj2j2Z38QwIMAICKXAfhfY8x/i8gMANch9A9gHIDZsWvml5PIQgudH3JbCm+71K0a6/JjvertmDfl1WHYlcq2f8V/rN7q7W3W55FnlV+Jt+rNRBSe8Mfzpd4quvLNOuW2SYoriJHSub2ymaNu+u7oBldsI6GxHper30CKntBJqnTPBCDuvpSQqv2O7XC/j6YoUbb+f51sde7P7rI67UtSkOJUqE1SzQMIPazLQ2gMP7VumkQIiQWnlFRjjFkIYGFYbwZwcd03iRASC5hBV49EFlrwCzTsfnGy1bPHfV/5XXzl96we0E5PIbXv2cfq/QddBt3Qe69Tfk90zLC6bIAeChw75rrCCd5WzJtfmaL8fvSIW2FWsvwz1IQkuS5z5NBFklxG3Zz5eiT41sNuq+TDe92wo0lqmvLb8bHLruv30mRl27CzsMZrBw3mxhMSEBjshAQEduMbEH5ttYu6drf6u/v3Kb/BaxZY/fgrOiMtb4PbgfSWnz1q9aLf5yq/Aq/mWuf2+kn60m9+27WpqXvifla/LOU34Aej3Xt+M13ZjjVxi1MSvOFKSrJeCIPF8618cKbegmnVrfe5g89yrOz72CPK75Mt663O/fldyna620Z9GeGdnZCAwGAnJCAw2AkJCByzN1A2/cWNX4dGpBmvydtk9dJVuuDkRf37WX3PAy4jL3LaqUObNjXaxnz/ZqtLN7jx8P1nZyi/P49203lfueUbyvZI5nlWHy5zxSv2RGS/PbAq1+r1V41Rtq+/5+rGN8p0zzBWmmPKr9KbsgxiZly08M5OSEBgsBMSENiNjzN+llzpgQPK9vYPb7H6imQ3HXZbeRPld+FSN131rTYtlO0WaWn1W8NHWH3V/HnKL8ErUBHZjZ/5gp4CO07OypXqeNc+NyW4+o/PKdvEdq6gxG8OusL3N99ztz5pnhsmLE/XmXFz/vUBqiOyvYnsukcF7+yEBAQGOyEBgcFOSECQeK4Eys7ONjk5OSd3/BJTWOJWlH3U9mxlu27cRKurjCsQmRA5Jm3qUlFRoos1bFzufr/dWrS2uvmKd5XfkYj90k6HQ8fcFNh7V+iVc0MauZ+trZdy+2GBHvc3SXPPI1pV6mcTFT9wU3s9brwR5ORkZ2cjJyen2rxg3tkJCQgMdkICAqfeYkCVVyd97969yrZmtVulduXsmcpW8pIr3pDWsq0zRPxL3u511SvKy5UtyVtVlujpqhgM15YuXWL1kd16X+mSDq6uXcFhNyS5dPBV+iSVXj37iDZOv3+S1X43/gsFMAK+mi1aeGcnJCAw2AkJCOzGxwA/O+36669Xthu97uitHy1StsHr863u0sN1/9tt36/8Oia6p9tpyXphyXufu/L939jhnnwf3VGo/OqiqEPrVu5p/8cRf0kp3u/gmPfUvuqoLludkOzeGLmF1PXXjrO6d1uXUbi8YJvyS03hnqLRwDs7IQGBwU5IQGCwExIQOGaPMZMmTVLHFd7Wyc89+ZSy7fCKQPbu0cPqwb16Kr8t6zdYrSf2gPWFbmy+2iskGYvpqn59XI36i/ZtV7aHMrOsLjxy2Or33n9D+V3YK9vqz4/oVYDpj7iMwrZvv2r1R4v0s44Rw4dH3+gAE+3+7PkADgGoBFBhjMkWkVYA/gEgA0A+gLHGmH01nYMQUr+cSjf+q8aYLGPM8X/FkwDMM8ZkApgXPiaENFBq040fDeCysJ6G0B5wD9SyPV86Ro4cWaNtZUQxiKuvvtpqv7u/bNky5de5c+caz+nXY/OJRZaZPzSoiJja69mho9XfPuR2gn3g8EHlN3fsTVYXrslTtq0v/93qN1Z+YnVRURHIqRPtnd0AeE9EPhGRCeHX2hljjn/CuwC0q/6thJCGQLR39qHGmB0i0hbAXBFRu/gZY4yIVJt8Hf7nMAEAunTpUqvGEkJOn6ju7MaYHeHvRQBeR2ir5t0i0gEAwt+r7VsZY6YYY7KNMdnpESWRCSHx46R3dhFpAiDBGHMorL8G4OcA3gQwDsAT4e+zaz4LqY6+ffuq44KCglqfM57FF/3nAJFTe+sLd1Tr9/OIc2wc5H4HQx+4XdkGe6nALVu6Qpoffvih8uvhTVOSmommG98OwOvhDywJwN+NMe+IyDIAr4rIeABbAYyNXTMJIbXlpMFujNkMoG81r+8FMOKL7yCENESYQUfqhMipPb9b7+u0NF0bvqlXn+6+u3VNeX8aMZ61Er+sMDeekIDAYCckIDDYCQkIHLOTmFDTtFxJRJ17H+7hFlt4ZyckIDDYCQkI7MaTmBPtijvWf48tvLMTEhAY7IQEBAY7IQGBwU5IQGCwExIQGOyEBAQGOyEBgcFOSEBgsBMSEBjshAQEBjshAYHBTkhAYLATEhAY7IQEBAY7IQGBwU5IQGCwExIQogp2EWkhIq+JyGcisk5EBolIKxGZKyIbw99bnvxMhJD6Ito7+7MA3jHG9EJoK6h1ACYBmGeMyQQwL3xMCGmgnDTYReQsAMMATAUAY0yZMWY/gNEApoXdpgG4NjZNJITUBdHc2bsCKAbwFxFZISJ/Dm/d3M4YUxj22YXQbq+EkAZKNMGeBKA/gD8YY/oBOIKILrsJVfevduc9EZkgIjkiklNcXFzb9hJCTpNogr0AQIExZmn4+DWEgn+3iHQAgPD3ourebIyZYozJNsZkp6en10WbCSGnwUmD3RizC8B2EekZfmkEgLUA3gQwLvzaOACzY9JCQkidEO0mEXcAeFlEUgBsBnAzQv8oXhWR8QC2AhgbmyYSQuqCqILdGJMLILsa04g6bQ0hJGYwg46QgMBgJyQgMNgJCQgMdkICAoOdkIDAYCckIDDYCQkIEkprj9PFRIoRSsBpA2BP3C5cPQ2hDQDbEQnboTnVdpxjjKk2Lz2uwW4vKpJjjKkuSSdQbWA72I54toPdeEICAoOdkIBQX8E+pZ6u69MQ2gCwHZGwHZo6a0e9jNkJIfGH3XhCAkJcg11ERonIehHJE5G4VaMVkRdEpEhEVnuvxb0Utoh0FpEFIrJWRNaIyMT6aIuIpIrIxyKyMtyOn4Vf7yoiS8Ofzz/C9QtijogkhusbzqmvdohIvoisEpFcEckJv1YffyMxK9set2AXkUQAvwNwJYDeAG4Qkd5xuvyLAEZFvFYfpbArANxrjOkNYCCA28K/g3i3pRTAcGNMXwBZAEaJyEAAvwLwjDHmXAD7AIyPcTuOMxGh8uTHqa92fNUYk+VNddXH30jsyrYbY+LyBWAQgHe94wcBPBjH62cAWO0drwfQIaw7AFgfr7Z4bZgNYGR9tgVAGoDlAAYglLyRVN3nFcPrdwr/AQ8HMAeA1FM78gG0iXgtrp8LgLMAbEH4WVpdtyOe3fiOALZ7xwXh1+qLei2FLSIZAPoBWFofbQl3nXMRKhQ6F8AmAPuNMRVhl3h9PpMB3A+gKnzcup7aYQC8JyKfiMiE8Gvx/lxiWradD+hw4lLYsUBEmgKYCeAuY8zB+miLMabSGJOF0J31YgC9Yn3NSETk6wCKjDGfxPva1TDUGNMfoWHmbSIyzDfG6XOpVdn2kxHPYN8BoLN33Cn8Wn0RVSnsukZEkhEK9JeNMbPqsy0AYEK7+yxAqLvcQkSO1yWMx+czBMA1IpIPYDpCXfln66EdMMbsCH8vAvA6Qv8A4/251Kps+8mIZ7AvA5AZftKaAuB6hMpR1xdxL4UtIoLQNlrrjDFP11dbRCRdRFqEdWOEnhusQyjor4tXO4wxDxpjOhljMhD6e5hvjPnveLdDRJqISLPjGsDXAKxGnD8XE+uy7bF+8BHxoOEqABsQGh8+HMfrvgKgEEA5Qv89xyM0NpwHYCOA9wG0ikM7hiLUBfsUQG7466p4twXABQBWhNuxGsAj4de7AfgYQB6AGQAaxfEzugzAnPpoR/h6K8Nfa47/bdbT30gWgJzwZ/MGgJZ11Q5m0BESEPiAjpCAwGAnJCAw2AkJCAx2QgICg52QgMBgJyQgMNgJCQgMdkICwv8HM5DtmZmAIVoAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "image_from_array(x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "balanced-colors",
   "metadata": {},
   "outputs": [],
   "source": [
    "encodings = model(x)[3].detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "suspected-retention",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.35 ms ± 181 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "image = encodings_to_image(encodings, encodings_mapping, image_size, mapping_size)\n",
    "# plt.imshow(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "electoral-wales",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.1 ms ± 277 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "image = model.quantize_and_decode(encodings, target_shape, device)\n",
    "# image_from_array(image, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "precious-qatar",
   "metadata": {},
   "source": [
    "Observations:\n",
    "The decoder is faster in cases where we have a lot of encodings E.G. 1024 or 4096. But in cases with a smaller number of embeddings (256, 64), the non-decoder approach is faster."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
